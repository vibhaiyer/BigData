{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: integer (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: float (nullable = true)\n",
      " |-- CustomerID: float (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf = pyspark.SparkConf().setAppName('SparkApp').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "retaildata = spark.read.csv(\n",
    "    path = \"2010-12-01.csv\",\n",
    "    sep = \",\",\n",
    "    header=True,\n",
    "    quote='\"',\n",
    "    schema=\"InvoiceNo INT, StockCode STRING, Description STRING,Quantity INT, InvoiceDate TIMESTAMP, UnitPrice FLOAT, CustomerID FLOAT, Country STRING\")\n",
    "\n",
    "retaildata.show()\n",
    "retaildata.printSchema()\n",
    "\n",
    "retaildata.registerTempTable(\"retaildata\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc,sum,count, col,year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sort [InvoiceNo#0 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(InvoiceNo#0 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#34]\n",
      "   +- FileScan csv [InvoiceNo#0,StockCode#1,Description#2,Quantity#3,InvoiceDate#4,UnitPrice#5,CustomerID#6,Country#7] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/2010-1..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:int,StockCode:string,Description:string,Quantity:int,InvoiceDate:timestamp,UnitP...\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Sort [InvoiceNo#0 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(InvoiceNo#0 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#46]\n",
      "   +- FileScan csv [InvoiceNo#0,StockCode#1,Description#2,Quantity#3,InvoiceDate#4,UnitPrice#5,CustomerID#6,Country#7] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/2010-1..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:int,StockCode:string,Description:string,Quantity:int,InvoiceDate:timestamp,UnitP...\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[InvoiceNo#0 DESC NULLS LAST], output=[InvoiceNo#0,StockCode#1,Description#2,Quantity#3,InvoiceDate#4,UnitPrice#5,CustomerID#6,Country#7])\n",
      "+- FileScan csv [InvoiceNo#0,StockCode#1,Description#2,Quantity#3,InvoiceDate#4,UnitPrice#5,CustomerID#6,Country#7] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/2010-1..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:int,StockCode:string,Description:string,Quantity:int,InvoiceDate:timestamp,UnitP...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retaildata.sort(\"InvoiceNo\").explain()\n",
    "retaildata.sort(desc(\"InvoiceNo\")).explain()\n",
    "retaildata.sort(desc(\"InvoiceNo\")).limit(5).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "CollectLimit 5\n",
      "+- FileScan csv [InvoiceNo#0,StockCode#1,Description#2,Quantity#3,InvoiceDate#4,UnitPrice#5,CustomerID#6,Country#7] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/2010-1..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:int,StockCode:string,Description:string,Quantity:int,InvoiceDate:timestamp,UnitP...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retaildata.limit(5).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[InvoiceNo#0], functions=[sum(cast(Quantity#3 as bigint))])\n",
      "+- Exchange hashpartitioning(InvoiceNo#0, 200), ENSURE_REQUIREMENTS, [id=#78]\n",
      "   +- *(1) HashAggregate(keys=[InvoiceNo#0], functions=[partial_sum(cast(Quantity#3 as bigint))])\n",
      "      +- FileScan csv [InvoiceNo#0,Quantity#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/2010-1..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:int,Quantity:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retaildata.groupBy(\"InvoiceNo\").sum(\"Quantity\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[InvoiceNo#0], functions=[sum(cast(Quantity#3 as bigint))])\n",
      "+- Exchange hashpartitioning(InvoiceNo#0, 200), ENSURE_REQUIREMENTS, [id=#97]\n",
      "   +- *(1) HashAggregate(keys=[InvoiceNo#0], functions=[partial_sum(cast(Quantity#3 as bigint))])\n",
      "      +- FileScan csv [InvoiceNo#0,Quantity#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/2010-1..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:int,Quantity:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retaildata.groupBy(\"InvoiceNo\").agg(sum(\"Quantity\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[InvoiceNo#0], functions=[sum(cast(Quantity#3 as bigint))])\n",
      "+- Exchange hashpartitioning(InvoiceNo#0, 200), ENSURE_REQUIREMENTS, [id=#116]\n",
      "   +- *(1) HashAggregate(keys=[InvoiceNo#0], functions=[partial_sum(cast(Quantity#3 as bigint))])\n",
      "      +- FileScan csv [InvoiceNo#0,Quantity#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/2010-1..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:int,Quantity:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retaildata.groupBy(\"InvoiceNo\").agg(sum(\"Quantity\")).withColumnRenamed(\"sum(miles)\", \"destination_total\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[InvoiceNo#0], functions=[count(Quantity#3)])\n",
      "+- Exchange hashpartitioning(InvoiceNo#0, 200), ENSURE_REQUIREMENTS, [id=#135]\n",
      "   +- *(1) HashAggregate(keys=[InvoiceNo#0], functions=[partial_count(Quantity#3)])\n",
      "      +- FileScan csv [InvoiceNo#0,Quantity#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/2010-1..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:int,Quantity:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retaildata.groupBy(\"InvoiceNo\").agg(count(\"Quantity\")).withColumnRenamed(\"sum(miles)\", \"destination_total\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Sort [InvoiceNo#0 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(InvoiceNo#0 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#165]\n",
      "   +- *(2) HashAggregate(keys=[InvoiceNo#0], functions=[count(Quantity#3)])\n",
      "      +- Exchange hashpartitioning(InvoiceNo#0, 200), ENSURE_REQUIREMENTS, [id=#161]\n",
      "         +- *(1) HashAggregate(keys=[InvoiceNo#0], functions=[partial_count(Quantity#3)])\n",
      "            +- FileScan csv [InvoiceNo#0,Quantity#3] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/2010-1..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<InvoiceNo:int,Quantity:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retaildata.groupBy(\"InvoiceNo\").agg(count(\"Quantity\")).withColumnRenamed(\"sum(miles)\", \"destination_total\").sort(\"InvoiceNo\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Sort [InvoiceNo#0 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(InvoiceNo#0 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#247]\n",
      "   +- *(2) HashAggregate(keys=[InvoiceNo#0], functions=[sum(cast(Quantity#3 as bigint))])\n",
      "      +- Exchange hashpartitioning(InvoiceNo#0, 200), ENSURE_REQUIREMENTS, [id=#243]\n",
      "         +- *(1) HashAggregate(keys=[InvoiceNo#0], functions=[partial_sum(cast(Quantity#3 as bigint))])\n",
      "            +- *(1) Project [InvoiceNo#0, Quantity#3]\n",
      "               +- *(1) Filter (isnotnull(Country#7) AND (Country#7 = United States))\n",
      "                  +- FileScan csv [InvoiceNo#0,Quantity#3,Country#7] Batched: false, DataFilters: [isnotnull(Country#7), (Country#7 = United States)], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/2010-1..., PartitionFilters: [], PushedFilters: [IsNotNull(Country), EqualTo(Country,United States)], ReadSchema: struct<InvoiceNo:int,Quantity:int,Country:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retaildata.filter((col(\"Country\").isNotNull()) & (col(\"Country\") == \"United States\")).groupBy(\"InvoiceNo\").sum(\"Quantity\").sort(\"InvoiceNo\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+-----------+\n",
      "|order_id|order_date|amount|customer_id|\n",
      "+--------+----------+------+-----------+\n",
      "|       1|07/04/1776|234.56|          1|\n",
      "|       2|03/14/1760|  78.5|          3|\n",
      "|       3|05/23/1784| 124.0|          2|\n",
      "|       4|09/03/1790|  65.5|          3|\n",
      "|       5|07/21/1795|  25.5|          5|\n",
      "|       6|11/27/1787|  14.4|          7|\n",
      "+--------+----------+------+-----------+\n",
      "\n",
      "+--------+------+---------+--------+---------+\n",
      "|order_id|itemid|productid|quantity|unitprice|\n",
      "+--------+------+---------+--------+---------+\n",
      "|       2|   123|     null|      12|     null|\n",
      "|       1|   456|     null|      34|     null|\n",
      "|       3|   789|     null|      56|     null|\n",
      "|       5|   123|     null|      12|     null|\n",
      "|       6|   456|     null|      34|     null|\n",
      "|       3|   789|     null|      12|     null|\n",
      "|       4|   123|     null|      12|     null|\n",
      "|       5|   456|     null|      56|     null|\n",
      "|       2|   789|     null|      12|     null|\n",
      "|       3|   123|     null|      12|     null|\n",
      "|       6|   456|     null|      56|     null|\n",
      "|       1|   789|     null|      56|     null|\n",
      "+--------+------+---------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"orders.csv\")\n",
    "orders.show()\n",
    "order_items = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"order_items.csv\")\n",
    "order_items.show()\n",
    "\n",
    "orders.registerTempTable(\"orders\")\n",
    "order_items.registerTempTable(\"order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Sort [year#474 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(year#474 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#507]\n",
      "   +- *(3) HashAggregate(keys=[year(cast(order_date#390 as date))#479], functions=[sum((cast(quantity#437 as double) * cast(unitprice#438 as double)))])\n",
      "      +- Exchange hashpartitioning(year(cast(order_date#390 as date))#479, 200), ENSURE_REQUIREMENTS, [id=#503]\n",
      "         +- *(2) HashAggregate(keys=[year(cast(order_date#390 as date)) AS year(cast(order_date#390 as date))#479], functions=[partial_sum((cast(quantity#437 as double) * cast(unitprice#438 as double)))])\n",
      "            +- *(2) Project [order_date#390, quantity#437, unitprice#438]\n",
      "               +- *(2) BroadcastHashJoin [order_id#389], [order_id#434], Inner, BuildRight, false\n",
      "                  :- *(2) Filter isnotnull(order_id#389)\n",
      "                  :  +- FileScan csv [order_id#389,order_date#390] Batched: false, DataFilters: [isnotnull(order_id#389)], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/orders..., PartitionFilters: [], PushedFilters: [IsNotNull(order_id)], ReadSchema: struct<order_id:int,order_date:string>\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#497]\n",
      "                     +- *(1) Filter isnotnull(order_id#434)\n",
      "                        +- FileScan csv [order_id#434,quantity#437,unitprice#438] Batched: false, DataFilters: [isnotnull(order_id#434)], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/order_..., PartitionFilters: [], PushedFilters: [IsNotNull(order_id)], ReadSchema: struct<order_id:int,quantity:int,unitprice:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT year(order_date) as year, sum(quantity * unitprice) as revenue FROM orders NATURAL JOIN order_items GROUP BY year(order_date) ORDER BY year\"\"\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Sort [year#769 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(year#769 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#730]\n",
      "   +- *(3) HashAggregate(keys=[year(cast(order_date#390 as date))#774], functions=[sum((cast(quantity#437 as double) * cast(unitprice#438 as double)))])\n",
      "      +- Exchange hashpartitioning(year(cast(order_date#390 as date))#774, 200), ENSURE_REQUIREMENTS, [id=#726]\n",
      "         +- *(2) HashAggregate(keys=[year(cast(order_date#390 as date)) AS year(cast(order_date#390 as date))#774], functions=[partial_sum((cast(quantity#437 as double) * cast(unitprice#438 as double)))])\n",
      "            +- *(2) Project [order_date#390, quantity#437, unitprice#438]\n",
      "               +- *(2) BroadcastHashJoin [order_id#389], [order_id#434], Inner, BuildRight, false\n",
      "                  :- *(2) Filter isnotnull(order_id#389)\n",
      "                  :  +- FileScan csv [order_id#389,order_date#390] Batched: false, DataFilters: [isnotnull(order_id#389)], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/orders..., PartitionFilters: [], PushedFilters: [IsNotNull(order_id)], ReadSchema: struct<order_id:int,order_date:string>\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#720]\n",
      "                     +- *(1) Filter isnotnull(order_id#434)\n",
      "                        +- FileScan csv [order_id#434,quantity#437,unitprice#438] Batched: false, DataFilters: [isnotnull(order_id#434)], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/order_..., PartitionFilters: [], PushedFilters: [IsNotNull(order_id)], ReadSchema: struct<order_id:int,quantity:int,unitprice:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select year(order_date) AS year, sum(quantity*unitprice)\n",
    "            from orders o INNER JOIN order_items i\n",
    "            ON o.order_id = i.order_id \n",
    "            GROUP BY year(order_date) \n",
    "            ORDER BY year\"\"\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(4) Sort [year#749 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(year#749 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#662]\n",
      "   +- *(3) HashAggregate(keys=[year(cast(order_date#390 as date))#763], functions=[sum((cast(quantity#437 as double) * cast(unitprice#438 as double)))])\n",
      "      +- Exchange hashpartitioning(year(cast(order_date#390 as date))#763, 200), ENSURE_REQUIREMENTS, [id=#658]\n",
      "         +- *(2) HashAggregate(keys=[year(cast(order_date#390 as date)) AS year(cast(order_date#390 as date))#763], functions=[partial_sum((cast(quantity#437 as double) * cast(unitprice#438 as double)))])\n",
      "            +- *(2) Project [order_date#390, quantity#437, unitprice#438]\n",
      "               +- *(2) BroadcastHashJoin [order_id#389], [order_id#434], Inner, BuildRight, false\n",
      "                  :- *(2) Filter isnotnull(order_id#389)\n",
      "                  :  +- FileScan csv [order_id#389,order_date#390] Batched: false, DataFilters: [isnotnull(order_id#389)], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/orders..., PartitionFilters: [], PushedFilters: [IsNotNull(order_id)], ReadSchema: struct<order_id:int,order_date:string>\n",
      "                  +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [id=#652]\n",
      "                     +- *(1) Filter isnotnull(order_id#434)\n",
      "                        +- FileScan csv [order_id#434,quantity#437,unitprice#438] Batched: false, DataFilters: [isnotnull(order_id#434)], Format: CSV, Location: InMemoryFileIndex[file:/F:/Koblenz/Study/2nd semester/Big Data/Exam 2/SQL and Explain Plan/order_..., PartitionFilters: [], PushedFilters: [IsNotNull(order_id)], ReadSchema: struct<order_id:int,quantity:int,unitprice:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.join(order_items,orders[\"order_id\"] == order_items[\"order_id\"],\"inner\").groupBy(year(col(\"order_date\")).alias(\"year\")).agg(sum(col(\"quantity\") * col(\"unitprice\"))).sort(\"year\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
